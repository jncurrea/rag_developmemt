{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain faiss-cpu openai tiktoken\n",
    "#!pip install langchain faiss-cpu openai tiktoken\n",
    "#!pip install pdfplumber\n",
    "#!pip install pdfminer.six\n",
    "#!pip install langchain openai faiss-cpu tiktoken\n",
    "#!pip install langchain-community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exctract Text from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "# Path to the PDF\n",
    "pdf_path = \"document.pdf\"\n",
    "# Extract text using pdfminer (better at preserving formatting)\n",
    "pdf_text = extract_text(pdf_path)\n",
    "# Save to a file\n",
    "with open(\"clean_document_text.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(pdf_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(pdf_text[:1000])  # Show first 1000 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28975"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pdf_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunk Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "with open(\"clean_document_text.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    pdf_text = f.read()\n",
    "\n",
    "# Split into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "chunks = text_splitter.split_text(pdf_text)\n",
    "\n",
    "# Save chunks for reference\n",
    "with open(\"document_chunks.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\\n---\\n\\n\".join(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 77\n",
      "\n",
      "Sample Chunk:\n",
      " customer in the year 2030. His digital personal assistant orders him an\n",
      "\n",
      "autonomous vehicle for a meeting across town. Upon hopping into the arriving car,\n",
      "\n",
      "Scott decides he wants to drive today and moves the car into “active” mode. Scott’s\n",
      "\n",
      "personal assistant maps out a potential route and shares it with his mobility insurer,\n",
      "\n",
      "which immediately responds with an alternate route that has a much lower likelihood\n",
      "\n",
      "of accidents and auto damage as well as the calculated adjustment to his monthly\n"
     ]
    }
   ],
   "source": [
    "# Print some chunks to verify\n",
    "print(f\"Total chunks created: {len(chunks)}\")\n",
    "print(\"\\nSample Chunk:\\n\", chunks[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR_API_KEY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index saved successfully!\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "# Store embeddings in FAISS\n",
    "vector_store = FAISS.from_texts(chunks, embedding=embeddings)\n",
    "\n",
    "# Save FAISS index for later use\n",
    "faiss_index_path = \"faiss_index\"\n",
    "vector_store.save_local(faiss_index_path)\n",
    "\n",
    "print(\"FAISS index saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query FAISS and use GPT04 to answer questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "# Load FAISS index\n",
    "faiss_index_path = \"faiss_index\"\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vector_store = FAISS.load_local(faiss_index_path, embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "# Function to retrieve relevant chunks from FAISS\n",
    "def retrieve_context(query, k=30):\n",
    "    \"\"\"\n",
    "    Retrieve the most relevant document chunks for the given query.\n",
    "    Ensures proper formatting to avoid vertical text issues.\n",
    "    \"\"\"\n",
    "    docs = vector_store.similarity_search(query, k=k)\n",
    "    \n",
    "    # ✅ Fix text formatting by stripping extra spaces & ensuring line breaks are correct\n",
    "    formatted_context = \"\\n\\n---\\n\\n\".join(\n",
    "        [doc.page_content.replace(\"\\n\", \" \").strip() for doc in docs]\n",
    "    )\n",
    "\n",
    "    return formatted_context\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "def ask_document(query):\n",
    "    \"\"\"\n",
    "    Uses FAISS retrieval + GPT-4 to answer user questions.\n",
    "    Ensures GPT-4 returns properly formatted responses.\n",
    "    \"\"\"\n",
    "    context = retrieve_context(query)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are an AI assistant that answers questions based only on the given document.\n",
    "    If the answer is not found in the document, say \"Sorry, no relevant information found.\"\n",
    "\n",
    "    Ensure your response is formatted correctly, using full sentences. \n",
    "    DO NOT return text one letter per line.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question: {query}\n",
    "\n",
    "    Answer:\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content.strip()  # Remove unnecessary spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The authors of the document are Ramnath Balasubramanian, Ari Libarikian, and Doug McElhaney.\n"
     ]
    }
   ],
   "source": [
    "# Test retrieval\n",
    "query = \"Who is the author of the document?\"\n",
    "retrieved_chunks = ask_document(query)\n",
    "print((retrieved_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document was written by three authors: Ramnath Balasubramanian, Ari Libarikian, and Doug McElhaney.\n"
     ]
    }
   ],
   "source": [
    "# Test retrieval\n",
    "query = \"How many authors wrote the document?\"\n",
    "retrieved_chunks = ask_document(query)\n",
    "print((retrieved_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document discusses the impact of AI on the future of the insurance industry as outlined by McKinsey & Company. It covers a range of topics including how AI will reshape claims processing, distribution, underwriting, and pricing. It also highlights the importance of preparing for these changes by adopting a strategic plan that integrates AI-related technologies, focuses on skill building, and includes regular milestones and checkpoints. The document further explores advancements in AI technologies, such as cognitive models, and their applications, as well as the evolving business landscape with the advent of IoT, blockchain, and other innovations. Additionally, it addresses the shift from \"detect and repair\" to \"predict and prevent\" in insurance processes, the need for effective data management strategies, and the development of new products and services tailored to future needs.\n"
     ]
    }
   ],
   "source": [
    "# Test retrieval\n",
    "query = \"What is the document talking about?\"\n",
    "retrieved_chunks = ask_document(query)\n",
    "print((retrieved_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The name of the document is \"Insurance 2030--The impact of AI on the future of insurance | McKinsey & Company\".\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the name of the document?\"\n",
    "retrieved_chunks = ask_document(query)\n",
    "print((retrieved_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document highlights the following three important ideas:\n",
      "\n",
      "1. **Integration of AI in the Insurance Industry**: The document emphasizes that as AI becomes more integrated into the insurance industry, carriers must adapt to changes in claims processing, distribution, and underwriting. AI will transform the industry from a \"detect and repair\" model to a \"predict and prevent\" one, fundamentally altering the way insurers operate.\n",
      "\n",
      "2. **Development of a Comprehensive Strategic Plan**: Insurers are encouraged to develop a strategic plan that encompasses all dimensions of analytics-based initiatives, ranging from data handling to cultural changes. This plan should include investments in skill-building, change management, and milestone tracking to adapt to evolving AI technologies and market shifts.\n",
      "\n",
      "3. **Future Technological Trends and Their Impact**: The document outlines the impact of upcoming technologies such as additive manufacturing, autonomous vehicles, and IoT on the insurance industry by 2030. These technologies will lead to new economic structures, risk assessments, and customer expectations, necessitating carriers to develop new products and strategies to remain competitive.\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the 3 most important ideas of the document?\"\n",
    "retrieved_chunks = ask_document(query)\n",
    "print((retrieved_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r-tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
